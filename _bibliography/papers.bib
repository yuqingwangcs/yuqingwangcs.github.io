---
---

@article{wang2023tram,
  abstract  = {Reasoning about time is essential for understanding the nuances of events described in natural language. Previous research on this topic has been limited in scope, characterized by a lack of standardized benchmarks that would allow for consistent evaluations across different studies. In this paper, we introduce TRAM, a temporal reasoning benchmark composed of ten datasets, encompassing various temporal aspects of events such as order, arithmetic, frequency, and duration, designed to facilitate a comprehensive evaluation of the temporal reasoning capabilities of large language models (LLMs). We conduct an extensive evaluation using popular LLMs, such as GPT-4 and Llama2, in both zero-shot and few-shot learning scenarios. Additionally, we employ BERT-based models to establish the baseline evaluations. Our findings indicate that these models still trail human performance in temporal reasoning tasks. It is our aspiration that TRAM will spur further progress in enhancing the temporal reasoning abilities of LLMs.},
  title     = {TRAM: Benchmarking Temporal Reasoning for Large Language Models},
  author    = {Yuqing Wang and Yun Zhao},
  journal   = {arXiv preprint arXiv:2310.00835},
  year      = {2023},
  abbr      = {arXiv},
  arxiv     = {2310.00835},
  code      = {https://github.com/eternityyw/tram-benchmark},
  selected  = {true}
}

@article{wang2023metacognitive,
  abstract  = {In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. While recent research on prompting has enhanced the reasoning capabilities of LLMs, a gap remains in further improving their understanding abilities. In this study, we introduce Metacognitive Prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. Our experiments involve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general natural language understanding (NLU) tasks from the GLUE and SuperGLUE benchmarks. Results indicate that, although GPT-4 consistently excels in most tasks, PaLM, when equipped with MP, approaches its performance level. Furthermore, across models and datasets, MP consistently outperforms existing prompting methods, including standard and chain-of-thought prompting. This study underscores the potential to amplify the understanding abilities of LLMs and highlights the benefits of mirroring human introspective reasoning in NLU tasks.},
  title     = {Metacognitive Prompting Improves Understanding in Large Language Models},
  author    = {Yuqing Wang and Yun Zhao},
  journal   = {arXiv preprint arXiv:2308.05342},
  year      = {2023},
  abbr      = {arXiv},
  arxiv     = {2308.05342},
  code      = {https://github.com/EternityYW/Metacognitive-Prompting},
  selected  = {true}
}

@article{wang2023empirical,
  abstract  = {The Segment Anything Model (SAM) is a foundation model for general image segmentation. Although it exhibits impressive performance predominantly on natural images, understanding its robustness against various image perturbations and domains is critical for real-world applications where such challenges frequently arise. In this study we conduct a comprehensive robustness investigation of SAM under diverse real-world conditions. Our experiments encompass a wide range of image perturbations. Our experimental results demonstrate that SAM's performance generally declines under perturbed images, with varying degrees of vulnerability across different perturbations. By customizing prompting techniques and leveraging domain knowledge based on the unique characteristics of each dataset, the model's resilience to these perturbations can be enhanced, addressing dataset-specific challenges. This work sheds light on the limitations and strengths of SAM in real-world applications, promoting the development of more robust and versatile image segmentation solutions.},
  title     = {An Empirical Study on the Robustness of the Segment Anything Model (SAM)},
  author    = {Yuqing Wang and Yun Zhao and Linda Petzold},
  journal   = {arXiv preprint arXiv:2305.06422},
  year      = {2023},
  abbr      = {arXiv},
  arxiv     = {2305.06422},
  code      = {https://github.com/EternityYW/SAM-Robustness},
  selected  = {false}
}

@article{wang2023large,
  abstract  = {Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks. Additionally, our in-depth error analysis on the challenging relation extraction task offers valuable insights into error distribution and potential avenues for improvement using SQP. Our study sheds light on the practical implications of employing LLMs in the specialized domain of healthcare, serving as a foundation for future research and the development of potential applications in healthcare settings.},
  title     = {Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding},
  author    = {Yuqing Wang and Yun Zhao and Linda Petzold},
  booktitle = {Machine Learning for Healthcare Conference},
  year      = {2023},
  publisher = {PMLR},
  abbr      = {MLHC},
  arxiv     = {2304.05368},
  code      = {https://github.com/EternityYW/LLM_healthcare},
  poster    = {Yuqing_MLHC2023_poster.pdf},
  selected  = {true}
}
