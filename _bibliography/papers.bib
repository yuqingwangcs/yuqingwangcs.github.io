@article{wang2024unveiling,
  abstract  = {The advancement of large language models (LLMs) has demonstrated strong capabilities across various applications, including mental health analysis. However, existing studies have focused on predictive performance, leaving the critical issue of fairness underexplored, posing significant risks to vulnerable populations. Despite acknowledging potential biases, previous works have lacked thorough investigations into these biases and their impacts. To address this gap, we systematically evaluate biases across seven social factors (e.g., gender, age, religion) using ten LLMs with different prompting methods on eight diverse mental health datasets. Our results show that GPT-4 achieves the best overall balance in performance and fairness among LLMs, although it still lags behind domain-specific models like MentalRoBERTa in some cases. Additionally, our tailored fairness-aware prompts can effectively mitigate bias in mental health predictions, highlighting the great potential for fair analysis in this field.},
  title     = {Unveiling and Mitigating Bias in Mental Health Analysis with Large Language Models},
  author    = {Yuqing Wang and Yun Zhao and Sara Alessandra Keller and Anne de Hond and Marieke M Buchem and Malvika Pillai and Tina Hernandez-Boussard},
  journal   = {arXiv preprint arXiv:2406.12033},
  year      = {2024},
  abbr      = {arXiv},
  arxiv     = {2406.12033},
  code      = {https://github.com/EternityYW/BiasEval-LLM-MentalHealth},
  selected  = {true}
}

@article{wang2023gemini,
  abstract  = {The burgeoning interest in Multimodal Large Language Models (MLLMs), such as OpenAI's GPT-4V(ision), has significantly impacted both academic and industrial realms. These models enhance Large Language Models (LLMs) with advanced visual understanding capabilities, facilitating their application in a variety of multimodal tasks. Recently, Google introduced Gemini, a cutting-edge MLLM designed specifically for multimodal integration. Despite its advancements, preliminary benchmarks indicate that Gemini lags behind GPT models in commonsense reasoning tasks. However, this assessment, based on a limited dataset (i.e., HellaSWAG), does not fully capture Gemini's authentic commonsense reasoning potential. To address this gap, our study undertakes a thorough evaluation of Gemini's performance in complex reasoning tasks that necessitate the integration of commonsense knowledge across modalities. We carry out a comprehensive analysis of 12 commonsense reasoning datasets, ranging from general to domain-specific tasks. This includes 11 datasets focused solely on language, as well as one that incorporates multimodal elements. Our experiments across four LLMs and two MLLMs demonstrate Gemini's competitive commonsense reasoning capabilities. Additionally, we identify common challenges faced by current LLMs and MLLMs in addressing commonsense problems, underscoring the need for further advancements in enhancing the commonsense reasoning abilities of these models.},
  title     = {Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models},
  author    = {Yuqing Wang and Yun Zhao},
  journal   = {arXiv preprint arXiv:2312.17661},
  year      = {2023},
  abbr      = {arXiv},
  arxiv     = {2312.17661},
  code      = {https://github.com/EternityYW/Gemini-Commonsense-Evaluation/},
  selected  = {true}
}

@article{wang2023prominet,
  abstract  = {Email is a widely used tool for business communication, and email marketing has emerged as a cost-effective strategy for enterprises. While previous studies have examined factors affecting email marketing performance, limited research has focused on understanding email response behavior by considering email content and metadata. This study proposes a Prototype-based Multi-view Network (PROMINET) that incorporates semantic and structural information from email data. By utilizing prototype learning, the PROMINET model generates latent exemplars, enabling interpretable email response prediction. The model maps learned semantic and structural exemplars to observed samples in the training data at different levels of granularity, such as document, sentence, or phrase. The approach is evaluated on two real-world email datasets: the Enron corpus and an in-house Email Marketing corpus. Experimental results demonstrate that the PROMINET model outperforms baseline models, achieving a ~3% improvement in F1 score on both datasets. Additionally, the model provides interpretability through prototypes at different granularity levels while maintaining comparable performance to non-interpretable models. The learned prototypes also show potential for generating suggestions to enhance email text editing and improve the likelihood of effective email responses. This research contributes to enhancing sender-receiver communication and customer engagement in email interactions.},
  title     = {PROMINET: Prototype-based Multi-View Network for Interpretable Email Response Prediction},
  author    = {Yuqing Wang and Prashanth Vijayaraghavan and Ehsan Degan},
  journal   = {In Conference on Empirical Methods in Natural Language Processing},
  year      = {2023},
  abbr      = {EMNLP},
  arxiv     = {2310.16753},
  selected  = {false}
}

@article{wang2023tram,
  abstract  = {Reasoning about time is essential for understanding the nuances of events described in natural language. Previous research on this topic has been limited in scope, characterized by a lack of standardized benchmarks that would allow for consistent evaluations across different studies. In this paper, we introduce TRAM, a temporal reasoning benchmark composed of ten datasets, encompassing various temporal aspects of events such as order, arithmetic, frequency, and duration, designed to facilitate a comprehensive evaluation of the temporal reasoning capabilities of large language models (LLMs). We conduct an extensive evaluation using popular LLMs, such as GPT-4 and Llama2, in both zero-shot and few-shot learning scenarios. Additionally, we employ BERT-based models to establish the baseline evaluations. Our findings indicate that these models still trail human performance in temporal reasoning tasks. It is our aspiration that TRAM will spur further progress in enhancing the temporal reasoning abilities of LLMs.},
  title     = {TRAM: Benchmarking Temporal Reasoning for Large Language Models},
  author    = {Yuqing Wang and Yun Zhao},
  journal   = {In Findings of the Association for Computational Linguistics: ACL},
  year      = {2024},
  abbr      = {ACL},
  arxiv     = {2310.00835},
  code      = {https://github.com/eternityyw/tram-benchmark},
  selected  = {true}
}

@article{xia2023advanced,
  abstract  = {This paper presents PathFinder and PathFinderPlus, two novel end-to-end computer vision frameworks designed specifically for advanced setting strategy classification in volleyball matches from a single camera view. Our frameworks combine setting ball trajectory recognition with a novel set trajectory classifier to generate comprehensive and advanced statistical data. This approach offers a fresh perspective for in-game analysis and surpasses the current level of granularity in volleyball statistics. In comparison to existing methods used in our baseline PathFinder framework, our proposed ball trajectory detection methodology in PathFinderPlus exhibits superior performance for classifying setting tactics under various game conditions. This robustness is particularly advantageous in handling complex game situations and accommodating different camera angles. Additionally, our study introduces an innovative algorithm for automatic identification of the opposing team's right-side (opposite) hitter's current row (front or back) during gameplay, providing critical insights for tactical analysis. The successful demonstration of our single-camera system's feasibility and benefits makes high-level technical analysis accessible to volleyball enthusiasts of all skill levels and resource availability. Furthermore, the computational efficiency of our system allows for real-time deployment, enabling in-game strategy analysis and on-the-spot gameplan adjustments.},
  title     = {Advanced Volleyball Stats for All Levels: Automatic Setting Tactic Detection and Classification with a Single Camera},
  author    = {Haotian Xia and Rhys Tracy and Yun Zhao and Yuqing Wang and Yuan-Fang Wang and Weining Shen},
  journal   = {In International Conference on Data Mining Workshop},
  year      = {2023},
  abbr      = {ICDMW},
  arxiv     = {2309.14753},
  selected  = {false}
}

@article{wang2023metacognitive,
  abstract  = {In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. While recent research on prompting has enhanced the reasoning capabilities of LLMs, a gap remains in further improving their understanding abilities. In this study, we introduce Metacognitive Prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. Our experiments involve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general natural language understanding (NLU) tasks from the GLUE and SuperGLUE benchmarks. Results indicate that, although GPT-4 consistently excels in most tasks, PaLM, when equipped with MP, approaches its performance level. Furthermore, across models and datasets, MP consistently outperforms existing prompting methods, including standard and chain-of-thought prompting. This study underscores the potential to amplify the understanding abilities of LLMs and highlights the benefits of mirroring human introspective reasoning in NLU tasks.},
  title     = {Metacognitive Prompting Improves Understanding in Large Language Models},
  author    = {Yuqing Wang and Yun Zhao},
  journal   = {In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year      = {2024},
  abbr      = {NAACL},
  arxiv     = {2308.05342},
  code      = {https://github.com/EternityYW/Metacognitive-Prompting},
  selected  = {true}
}

@phdthesis{wang2023ai,
  title     = {AI and Big Data in Health: Boosting Reliability and Efficiency in Predictive Healthcare Models},
  author    = {Yuqing Wang},
  year      = {2023},
  school    = {University of California, Santa Barbara},
  abbr      = {Dissertation},
  pdf       = {Yuqing_thesis.pdf}
}

@article{wang2023empirical,
  abstract  = {The Segment Anything Model (SAM) is a foundation model for general image segmentation. Although it exhibits impressive performance predominantly on natural images, understanding its robustness against various image perturbations and domains is critical for real-world applications where such challenges frequently arise. In this study we conduct a comprehensive robustness investigation of SAM under diverse real-world conditions. Our experiments encompass a wide range of image perturbations. Our experimental results demonstrate that SAM's performance generally declines under perturbed images, with varying degrees of vulnerability across different perturbations. By customizing prompting techniques and leveraging domain knowledge based on the unique characteristics of each dataset, the model's resilience to these perturbations can be enhanced, addressing dataset-specific challenges. This work sheds light on the limitations and strengths of SAM in real-world applications, promoting the development of more robust and versatile image segmentation solutions.},
  title     = {An Empirical Study on the Robustness of the Segment Anything Model (SAM)},
  author    = {Yuqing Wang and Yun Zhao and Linda Petzold},
  journal   = {Pattern Recognition},
  year      = {2024},
  abbr      = {<span class="abbr">Pattern Recognit.</span>},
  arxiv     = {2305.06422},
  code      = {https://github.com/EternityYW/SAM-Robustness},
  selected  = {true}
}

@article{wang2023large,
  abstract  = {Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks. Additionally, our in-depth error analysis on the challenging relation extraction task offers valuable insights into error distribution and potential avenues for improvement using SQP. Our study sheds light on the practical implications of employing LLMs in the specialized domain of healthcare, serving as a foundation for future research and the development of potential applications in healthcare settings.},
  title     = {Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding},
  author    = {Yuqing Wang and Yun Zhao and Linda Petzold},
  journal   = {In Machine Learning for Healthcare Conference},
  year      = {2023},
  publisher = {PMLR},
  abbr      = {MLHC},
  arxiv     = {2304.05368},
  code      = {https://github.com/EternityYW/LLM_healthcare},
  poster    = {Yuqing_MLHC2023_poster.pdf},
  selected  = {true}
}

@inproceedings{wang2022predicting,
  abstract  = {As critically ill patients frequently develop anemia or coagulopathy, transfusion of blood products is a frequent intervention in the Intensive Care Units (ICU). However, inappropriate transfusion decisions made by physicians are often associated with increased risk of complications and higher hospital costs. In this work, we aim to develop a decision support tool that uses available patient information for transfusion decision-making on three common blood products (red blood cells, platelets, and fresh frozen plasma). To this end, we adopt an off-policy batch reinforcement learning (RL) algorithm, namely, discretized Batch Constrained Q-learning, to determine the best action (transfusion or not) given observed patient trajectories. Simultaneously, we consider different state representation approaches and reward design mechanisms to evaluate their impacts on policy learning. Experiments are conducted on two real-world critical care datasets: the MIMIC-III and the UCSF. Results demonstrate that policy recommendations on transfusion achieved comparable matching against true hospital policies via accuracy and weighted importance sampling evaluations on the MIMIC-III dataset. Furthermore, a combination of transfer learning (TL) and RL on the data-scarce UCSF dataset can provide up to 17.02% improvement in terms of accuracy, and up to 18.94% and 21.63% improvement in jump-start and asymptotic performance in terms of weighted importance sampling averaged over three transfusion tasks. Finally, simulations on transfusion decisions suggest that the transferred RL policy could reduce patients' estimated 28-day mortality rate by 2.74% and decreased acuity rate by 1.18% on the UCSF dataset. In short, RL with appropriate patient state encoding and reward designs shows promise in treatment recommendations for blood transfusion and further optimizes the real-time treatment strategies by improving patients' clinical outcomes.},
  title     = {Predicting the Need for Blood Transfusion in Intensive Care Units with Reinforcement Learning},
  author    = {Yuqing Wang and Yun Zhao and Linda Petzold},
  booktitle = {ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
  pages     = {1--10},
  year      = {2022},
  abbr      = {ACM-BCB},
  award     = {Best Student Paper Award},
  arxiv     = {2206.14198},
  selected  = {true}
}

@article{wang2022integrating,
  abstract  = {Sepsis is a leading cause of death in the Intensive Care Units (ICU). Early detection of sepsis is critical for patient survival. In this paper, we propose a multimodal Transformer model for early sepsis prediction, using the physiological time series data and clinical notes for each patient within 36 hours of ICU admission. Specifically, we aim to predict sepsis using only the first 12, 18, 24, 30 and 36 hours of laboratory measurements, vital signs, patient demographics, and clinical notes. We evaluate our model on two large critical care datasets: MIMIC-III and eICU-CRD. The proposed method is compared with six baselines. In addition, ablation analysis and case studies are conducted to study the influence of each individual component of the model and the contribution of each data modality for early sepsis prediction. Experimental results demonstrate the effectiveness of our method, which outperforms competitive baselines on all metrics.},
  title     = {Integrating Physiological Time Series and Clinical Notes with Transformer for Early Prediction of Sepsis},
  author    = {Yuqing Wang and Yun Zhao and Rachael Callcut and Linda Petzold},
  journal   = {In Industrial Conference on Data Mining},
  year      = {2022},
  abbr      = {ICDM},
  award     = {Best Paper Nominee},
  arxiv     = {2203.14469},
  selected  = {false}
}

@article{wang2022enhancing,
  abstract  = {Most current multivariate time series (MTS) classification algorithms focus on improving the predictive accuracy. However, for large-scale (either high-dimensional or long-sequential) time series (TS) datasets, there is an additional consideration: to design an efficient network architecture to reduce computational costs such as training time and memory footprint. In this work we propose a methodology based on module-wise pruning and Pareto analysis to investigate the relationship between model efficiency and accuracy, as well as its complexity. Comprehensive experiments on benchmark MTS datasets illustrate the effectiveness of our method.},
  title     = {Enhancing Transformer Efficiency for Multivariate Time Series Classification},
  author    = {Yuqing Wang and Yun Zhao and Linda Petzold},
  journal   = {In Industrial Conference on Data Mining},
  year      = {2022},
  abbr      = {ICDM},
  award     = {Best Paper Nominee},
  arxiv     = {2203.14472},
  selected  = {false}
}

@article{zhao2021empirical,
  abstract  = {COVID-19 has been a public health emergency of international concern since early 2020. Reliable forecasting is critical to diminish the impact of this disease. To date, a large number of different forecasting models have been proposed, mainly including statistical models, compartmental models, and deep learning models. However, due to various uncertain factors across different regions such as economics and government policy, no forecasting model appears to be the best for all scenarios. In this paper, we perform quantitative analysis of COVID-19 forecasting of confirmed cases and deaths across different regions in the United States with different forecasting horizons, and evaluate the relative impacts of the following three dimensions on the predictive performance (improvement and variation) through different evaluation metrics: model selection, hyperparameter tuning, and the length of time series required for training. We find that if a dimension brings about higher performance gains, if not well-tuned, it may also lead to harsher performance penalties. Furthermore, model selection is the dominant factor in determining the predictive performance. It is responsible for both the largest improvement and the largest variation in performance in all prediction tasks across different regions. While practitioners may perform more complicated time series analysis in practice, they should be able to achieve reasonable results if they have adequate insight into key decisions like model selection.},
  title     = {Empirical Quantitative Analysis of COVID-19 Forecasting Models},
  author    = {Yun Zhao and Yuqing Wang and Junfeng Liu and Haotian Xia and Zhenni Xu and Qinghang Hong and Zhiyang Zhou and Linda Petzold},
  journal   = {In International Conference on Data Mining Workshop},
  year      = {2021},
  abbr      = {ICDMW},
  award     = {Best Paper Award},
  arxiv     = {2110.00174},
  selected  = {true}
}

@article{wang2021empirical,
  abstract  = {Multiple organ failure (MOF) is a life-threatening condition. Due to its urgency and high mortality rate, early detection is critical for clinicians to provide appropriate treatment. In this paper, we perform quantitative analysis on early MOF prediction with comprehensive machine learning (ML) configurations, including data preprocessing (missing value treatment, label balancing, feature scaling), feature selection, classifier choice, and hyperparameter tuning. Results show that classifier choice impacts both the performance improvement and variation most among all the configurations. In general, complex classifiers including ensemble methods can provide better performance than simple classifiers. However, blindly pursuing complex classifiers is unwise as it also brings the risk of greater performance variation.},
  title     = {Empirical Analysis of Machine Learning Configurations for Prediction of Multiple Organ Failure in Trauma Patients},
  author    = {Yuqing Wang and Yun Zhao and Rachael Callcut and Linda Petzold},
  journal   = {In Industrial Conference on Data Mining},
  year      = {2021},
  abbr      = {ICDM},
  arxiv     = {2103.10929},
  selected  = {false}
}

@article{zhao2021bertsurv,
  abstract  = {Survival analysis is a technique to predict the times of specific outcomes, and is widely used in predicting the outcomes for intensive care unit (ICU) trauma patients. Recently, deep learning models have drawn increasing attention in healthcare. However, there is a lack of deep learning methods that can model the relationship between measurements, clinical notes and mortality outcomes. In this paper we introduce BERTSurv, a deep learning survival framework which applies Bidirectional Encoder Representations from Transformers (BERT) as a language representation model on unstructured clinical notes, for mortality prediction and survival analysis. We also incorporate clinical measurements in BERTSurv. With binary cross-entropy (BCE) loss, BERTSurv can predict mortality as a binary outcome (mortality prediction). With partial log-likelihood (PLL) loss, BERTSurv predicts the probability of mortality as a time-to-event outcome (survival analysis). We apply BERTSurv on Medical Information Mart for Intensive Care III (MIMIC III) trauma patient data. For mortality prediction, BERTSurv obtained an area under the curve of receiver operating characteristic curve (AUC-ROC) of 0.86, which is an improvement of 3.6% over baseline of multilayer perceptron (MLP) without notes. For survival analysis, BERTSurv achieved a concordance index (C-index) of 0.7. In addition, visualizations of BERT's attention heads help to extract patterns in clinical notes and improve model interpretability by showing how the model assigns weights to different inputs.},
  title     = {BERTSurv: BERT-Based Survival Models for Predicting Outcomes of Trauma Patients},
  author    = {Yun Zhao and Qinghang Hong and Xinlu Zhang and Yu Deng and Yuqing Wang and Linda Petzold},
  journal   = {In Industrial Conference on Data Mining},
  year      = {2021},
  abbr      = {ICDM},
  arxiv     = {2103.10928},
  selected  = {false}
}











